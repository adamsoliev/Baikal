https://github.com/rswier/swieros

Compilers
https://github.com/robertmuth/Cwerg/tree/master

Testing
https://github.com/nlsandler/write_a_c_compiler

## Notes

In some important sense, the compiler is all about representing
programs and analyzing and transforming them with the goal of improving
some metric (e.g. performance)

- REPRESENTING PROGRAMS

  - Conrete Syntax
  - Abstract Syntax (AST)
  - Instructions (and higher-level data structures built from them)

  – Control Flow Graph (may contain RTL instructions)
  – Dataflow Graph
  – Static Single Assignment (SSA)
  – Control Dependence Graph (CDG)
  – Program Dependence Graph (PDG)
  – Call Graph

  - Components of representation

    - Control dependencies (CFG, CDG)
    - Data dependencies (def/use chains, SSA)
    - PDG: super-impose dataflow graph (in SSA form or not) on top of CDG

  - Input, output, analysis and transformation issues

- ANALYZING AND TRANSFORMING PROGRAMS

  - Dataflow Analysis
    - Reaching definitions

  - Interprocedural analysis 
    - Do inlining, devirtualization and tail call optimizations first
    - Interprocedural analysis using CF super graph (call graph + CFGs of procedures) or summaries
      - context sensitive
      - context insensitive
    - Specialization

  - Pointer analysis

  - Intraprocedural Optimizations
    - Local (single block) analysis and optzs
    - Global (single function) analysis and optzs
  - Interprocedural Optimizations (program level)
    - Alias Analysis
  

two subsets of CFGs known as LL(1) and LR(1) grammars

IR
------------------
- Easy to process, such as create, read, write, traverse, or transfer.
- Easy to optimize, for example, allowing target-independent low-level optimizations

Graphical IRs: Parse trees, ASTs, and directed acyclic graphs (DAGs) 
Linear IRs: Stack machines, three address code (3AC)
Hybrid IRs: CFGs, 

Local optimizations
  Constant folding 
  Algebraic simplifications 
  Dead code elimination (DCE) 

Global optimizations
  Function inlining 
  Common-subexpression elimination (CSE) 
  Constant propagation (CP) 

Structural Organization
  leads to passes organized as some form of tree-walk
Level of Abstraction
  implicit (abstract) things in an IR are hard to optimize/work with
Mode of Use
  Definitive (main) vs derivative (temp and for a spec purpose)

GRAPHICAL IRs
    Syntax-Related Trees: 
        Parse tree
            most used in parsing discussions :)
        AST
            good for optimizations that rearrange expressions, tree-height balancing and tree-pattern matching
        DAG
            good for memory-sensitive systems and for exposing redundancies. usually derivative
    Graphs
        CFG
            consists of a graph of basic blocks (which could be expression-level AST, a DAG, or a linear IR)
            many parts (program analysis, instruction schedulers, register allocation) of compiler rely on them 
        Dependence Graph
            usually derivative IR built for a spec purpose
            important in instruction scheduling and parallelization optimizations
        Call Graph
            always a derivative IR built to support interprocedural analysis and optimization
LINEAR IRs
    Stack-machine code 
        often used as a definitive IR to transmit code between systems and environments
        compact programs and a simple scheme to port the language to new machines
    Three-address code
        often used as a definitive IR
        with its explicit name space and its L-S memory model, is well suited to optimization for R-to-R, L-S machines
        level of abstraction varies from low to high (if hardware supports exsotic operations)
    * Representing Linear Codes
        SIMPLE ARRAY vs ARRAY OF POINTERS vs LINKED LIST (of quadruples)
        may use different implementations to represent the IR at different points
    * Building the CFG from Linear Code
        find the first (leaders) and last instruction of each basic block and add edges between them
  
NAME SPACES (created in the IR) [meaning, giving names to definitions, expressions, etc]
    Use explicit names for things you want to optimize
    SSA: 
        (1) each definition has a unique name
        (2) each use refers to exactly one definition
        IR -> SSA form: 
            Insert φ-functions at points where CF paths merge and rename variables so that the SA property holds
            It works by selecting the value that corresponds to the edge along which control entered the block

PLACEMENT OF VALUES IN MEMORY


Appendix A: IR
ILOC

LLVM IR
https://mukulrathi.com/create-your-own-programming-language/llvm-ir-cpp-api-tutorial/
https://www.cs.cmu.edu/afs/cs/academic/class/15745-s13/public/lectures/L6-LLVM-Detail-1up.pdf

LLVM IR stack allo => virtual registers
https://stackoverflow.com/questions/46513801/llvm-opt-mem2reg-has-no-effect

High level constructs to LLVM IR 
https://buildmedia.readthedocs.org/media/pdf/mapping-high-level-constructs-to-llvm-ir/latest/mapping-high-level-constructs-to-llvm-ir.pdf

A Survey of Compiler Testing
https://sci-hub.ru/https://dl.acm.org/doi/fullHtml/10.1145/3363562

Previous interesting compilers
https://en.wikipedia.org/wiki/PL/8

COMPILER TESTING
https://stackoverflow.com/questions/1104922/compiler-test-cases-or-how-to-test-a-compiler
https://gcc.gnu.org/onlinedocs//gccint/C-Tests.html
https://software-dl.ti.com/ccs/esd/documents/sdto_cgt_compiler_validations.html
https://users.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf

https://github.com/c-testsuite/c-testsuite/tree/master
https://github.com/nlsandler/write_a_c_compiler
https://github.com/llvm/llvm-test-suite/tree/main/SingleSource/UnitTests
https://github.com/csmith-project/creduce [reduce large C/C++ programs into small ones that have the same behavior]
https://github.com/csmith-project/csmith [random generator of C programs]

Challenges
1. One challenge is the lack of a formal specification of what exactly a compiler is supposed to do
2. Another challenge for testing is the semantic richness of the input and output languages that compilers deal with
3. A third important challenge is that compilers have various options and features

Properties that simplify the problem of validating their correctness
1. One such property is that the inputs to compilers are written in a programming language, i.e., the space of possible inputs is clearly defined by the language grammar
2. Another property that eases compiler testing is that the semantics of the source language are usually specified
3. The fact that for most popular programming languages, there are multiple supposedly equivalent implementations, which compiler testing can exploit as an oracle for differential testing 

Notes on existing compilers
(llvm)[https://github.com/llvm/llvm-project]
  ---------------
  scanner
  ---------------
  switch(char) scanning while collecting meta info

  ---------------
  parser 
  ---------------
  ClangAST
  LLVM IR (SSN-form)
    Module 
      GlobalVariable 
      Function  (decl & def)
        BasicBlock
          Instructions
            CmpInst
          ...
  SelectionDAG

  ---------------
  semantic routines 
  ---------------

  ---------------
  optimizations 
  ---------------
  LLVM MIR

  ---------------
  code generation 
  ---------------
    Instruction Selection (SelectionDAG, FastlSel, GlobalISel)
    Register Allocation (RegAllocFast or RegAllocGreedy)
    Instruction Scheduling
    Machine IR
    Machine Optimizations
    Machine Specific Features

(TCC)[https://bellard.org/tcc/tcc-doc.html]
********* LEX
  BufferedFile  
  tcc_open(), tcc_close()
  inp() => next char
  next() => next token | next_nomacro() => next token without macro expansion
    switch(char) scanning while hashing it and putting it into symbol table
  TOK_xxx 

********* PARSE
  parsing expr is similar to grammar
  
  type_decl()
    handle pointer related logic
    if abstract
    else direct 
    post_type() // function or array

  decl()
    while (1)
      parse_btype()
      check for struct | enum
      while (1) // iterature each declaration
        type_decl()
        if function
          assertions
        check for assembly 
        function body

********* OTHER PARTS
  tcc generates code while parsing :)


(chibicc)[https://github.com/rui314/chibicc]
********* LEX
  ident | punct | keyword | str | num | pp_num | eof
  linkedlist of tokens
  Token -----------------------------
    | if num: int64_t | long double
    | if str: char *str, int len
    | file location | filename | line number | column number

********* PARSE
  linkedlist of Objs (global var | local var | function)
  Obj -------------------------------
    | char *name | Type *ty | Token *tok | bool is_local | int align | int offset 
    | Obj *params | Node *body | Obj *locals | int stack_size
  
  AST
  Node -------------------------------
    | Type *ty | Token *tok
    | if expr: Node *lhs | Node *rhs
    | if stmt: Node *cond | Node *then | Node *els | Node *init | Node *inc | Node *body
    | if num: int64_t val | long double fval
    | if var: Obj *var
    | if funcal: Type *func_ty | Node *args | bool pass_by_stack | Obj *ret_buffer
  
  Type -------------------------------
    | enum { INT, CHAR, PTR, ARRAY, STRUCT, FUNC, BOOL, VOID, ENUM, SHORT, LONG, FLOAT, DOUBLE, LDOUBLE, VLA, UNION }
    | int size | int align | bool is_unsigned | Type *origin | Type *base
    | if DECL: Token *name | Token *name_pos
    | if ARRAY: int array_len
    | if STRUCT: Member *members | bool is_flexible | bool is_packed
    | if FUNC: Type *return_ty | Type *params | bool is_variadic | Type *next
  
  Member -----------------------------
    | Member *next | Type *ty | Token *tok | Token *name | int align | int offset | int idx

********* CODE GENERATION 
  Assign lvar offset
    passed by stack parameters
    passed by registers parameters and local variables
  Emit data
    .data or .tdata
    .bss or .tbss
  Emit text
    ignore dead inline functions
    prologue
    save passed-by-register arguments to stack
    emit code for each stmt in the body
    if main, special handling
    epilogue


(shecc)[https://github.com/jserv/shecc]
(lcc)[https://github.com/drh/lcc]
********* LEX
********* PARSE
  program() // parses translation units
    while (EOF)
      decl(dclglobal)   // # function defition is a declaration with comp stmt
                        // # global vs local vs param declarations
        specifiers()    // declspec
                          // # 1st declarator is treated diffly bc it is used to recog func definition
        if (global) funcdefn()
          dclr()          // bty -> declarator -> full type
            dclr1()       // inverts the type so that it is easier to parse in dclr()
        else dclr()
        while (1)
          other declarations (dclglobal | dcllocal | dclparam)    // # param is also a decl
  dclr1()
    if ID
    if *
      dclr1()
    if (
      if abstract function
      else parameters()
    if [ 


(lacc)[https://github.com/larmel/lacc]
(cproc)[https://github.com/michaelforney/cproc]
(scc)[http://www.simple-cc.org/]
(SmallerC)[https://github.com/alexfru/SmallerC]
(kefir)[https://github.com/protopopov1122/kefir]


Memory Management

Symbol tables

Type representation
  challenge here is representing constructed types, chained from combo of basic types
    e.g., pointer to pointer to pointer to char
          function that take in a function pointer and returns int pointer
  - bit strings (tcc, pcc)
  - ll | graphs of structs (lcc, chibicc)

Lex

Parse

Codegen
frontend -> 

Optmizaton