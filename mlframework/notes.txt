



/* Robotics */
    -- sensors
    Range sensors (sonar, stereo vision, optical range, lidar, radar)
    Location sensors (gps, marker-based)
    Proprioceptive sensors (shaft, odometry, inertia, force and torque sensors)

    -- motion: actuator (transmissions, gears, etc) -> effector (legs, etc) 

    -- what problem robot is solving 
    when robots act in isolation and know their environment, the problem they
    are solving can be formulated as an MDP; when they are missing information it becomes a
    POMDP; and when they act around people it can often be formulated as a game

    -- to actually solve it, 3-level hierarchy is used
    task planning    - high level goal; e.g, kick a ball
    motion planning  - step by step action to achieve above goal
    control          - e.g., what torque to apply to actuators to complete one step in motion

    -- robotic perception (sensors outputs -> representation of env (inc. robot))
    given current state of env, robot makes observations  and takes action; to keep 
    track of those, we need corresponding sensor and motion models. Since robot 
    operates in nondeterministic env, both models and resulting state are wrapped 
    with probability distrubution

        -- localization and mapping
        * simple motion model can be derived from (x, y, 0) of robot; to be specific, 
        (x, y, 0) -> (v, w) velocities (translational, rotational) + normal distrubution
        to account for physical uncertainties
        * for sensor model one can use landmark or range-scan model 
        * when map of env exists, localization is done using either particle filtering or Kalman filter 
        * when map doesn't exist, SLAM can be used
        * other types of perception: temperature, sound, etc; reactive agent as opposed to reasoning one

        -- ML in robotic perception
        * ML enables robot to learn sensor and motion models from data and discover a fitting
        internal representation

    -- planning and control
    
        -- configuration space, where all the points that comprise the robot are represented 
        as a single point in an abstract multidimensional space
        
        -- motion planning - finding a plan that takes a robot from one configuration to another without colliding 
        with an obstacle

            -- spaces of motion planning
            * workspace or world W, where points are points in the everyday three-dimensional world
            * space of configurations, C, where points q in C are d-dimensional, with d the robot’s 
            number of degrees of freedom, and map to sets of points A(q) in W
            * space of paths, which is a space of functions. Each point in this space maps to 
            an entire curve through C-space. This space is ∞-dimensional

            -- solutions to motion planning problem
            * visibility graphs
            * voronoi diagrams
            * cell decomposition 
            * randomized motion planning (PRM or its extension RRT or modified RRT*)

        -- trajectory tracking control
            -- find how much torque you need and compensate for error with following control laws
                * P controller      (compensate for the error proportionally)
                * PD controller     (P + compensate more if it is increasing)
                * PID controller    (P + PD + compensate harder if you haven't made any progress in a long time)

        -- optimal control - find a sequence of actions (torques really) directly that, when executed by the robot, 
        result in state-action pairs with low cumulative cost
            * LQR
            * iterative LQR
    
    -- RL in robotics
        * model-based RL (essentially using physics-based sensor and motion models alongside RL)
        * end-to-end learning (take in sensor data and output torque values to move actuators)
        * sub-goal learning (learn sub-goals and then how to combine them)
        * transfer learning (reusing info from prev learning episodes on other tasks)
        * imitation learning (autonomous robot driving a car like a human)
        * adversarial learning (train a model to differentiate robot's learned vs human policies (the classifier) 
        and then train a new model to fool the the classifier)
